{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import logging\n",
    "import gensim\n",
    "import pymorphy2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from tqdm import tqdm_notebook\n",
    "from matplotlib import pyplot as plt\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from laserembeddings import Laser\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import manhattan_distances\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (30, 30)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('two_chats_df_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('two_chats_df_train.csv')\n",
    "df_train.shape\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_false_samples(df_input, n):\n",
    "    return df_input.loc[random.sample(list(df_input.index), n)] ## ЗАФИКСИРОВАТЬ СИД"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_test(df_input, n):\n",
    "    df_output = df_input.copy()\n",
    "    false_samples_list = [get_false_samples(df_output, n)['NContext'].values\n",
    "                          for each_sample in tqdm_notebook(df_output['NContext'])]\n",
    "    df_output['False_Samples'] = false_samples_list\n",
    "    return df_output[['NContext', 'NResponse', 'False_Samples']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = get_df_test(df_train, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['With_False_Samples'] = [[df_test['NResponse'][x]] +\n",
    "                                 list(df_test['False_Samples'][x]) for x in range(len(df_test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test.tail(-5).values\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "sentence_split_by_dot_pattern = r'(?<!\\s\\w\\d\\.)(?<!\\w\\.\\w\\.)(?<=[….?!])[\\s+\\n+]\\s*(?=[\\w\\\"\\'«]|<\\/?\\s*p\\s*>|-(?=\\s*\\w+))'\n",
    "sentence_split_pattern = sentence_split_by_dot_pattern \\\n",
    "    + r'|(?<![….!?]\\s<\\/p>)[\\s+\\n+]\\s*(?=<\\s*p\\s*>|<\\s*br\\s*\\/?>)|[….?!](?=<\\/?\\s*p\\s*>|<br\\s*\\/?>)'\n",
    "word_split_pattern = r\"(?P<word>(?:(?!_)(?:[\\w/]|(?<=\\w)[-'.](?=\\w)))+)\"\n",
    "sentence_split_regexp = re.compile(sentence_split_pattern, flags=re.UNICODE)\n",
    "word_split_regexp = re.compile(word_split_pattern, flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def split_text_into_part(text, part_regexp):\n",
    "    return [text for i, text in\n",
    "            enumerate(part_regexp.split(text))]\n",
    "\n",
    "\n",
    "def split_text_into_sentences(text):\n",
    "    return split_text_into_part(text, sentence_split_regexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "morph2 = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "conv_pos2 = {'ADJF': 'ADJ', 'ADJS': 'ADJ', 'ADV': 'ADV', 'NOUN': 'NOUN',\n",
    "             'VERB': 'VERB', 'PRTF': 'ADJ', 'PRTS': 'ADJ', 'GRND': 'VERB'}\n",
    "\n",
    "nones2 = {} \n",
    "tmp_dict2 = {} \n",
    "\n",
    "def normalizePymorphy_sentences(text, need_pos=False):\n",
    "    output = []\n",
    "    sentences = split_text_into_sentences(text)\n",
    "    for sentence in sentences:\n",
    "        tokens = re.findall(\n",
    "            '[A-Za-zА-Яа-яЁё]+\\-[A-Za-zА-Яа-яЁё]+|[A-Za-zА-Яа-яЁё]+|[0-9]+', sentence)\n",
    "        with open(\"stopwords.txt\", encoding=\"utf-8\") as file:\n",
    "            stop_words = file.read()\n",
    "        words = []\n",
    "        for t in tokens:\n",
    "            if t in tmp_dict2.keys():\n",
    "                words.append(tmp_dict2[t])\n",
    "            elif t in nones2.keys():\n",
    "                pass\n",
    "            else:\n",
    "                pv = morph2.parse(t)\n",
    "                # if pv[0].tag.POS != None and pv[0].score >= 0.20:\n",
    "                # if pv[0].tag.POS != None:\n",
    "                # pv[0].normal_form not in stop_words and \\\n",
    "                #  and pv[0].normal_form not in stop_words\n",
    "                if len(pv[0].normal_form) > 1:\n",
    "                    # and pv[0].tag.POS in conv_pos2.keys()\n",
    "                    # if pv[0].normal_form != 'быть' and \\\n",
    "                    # if pv[0].tag.POS in conv_pos.keys():\n",
    "                    if need_pos:\n",
    "                        word = pv[0].normal_form+\"_\"+conv_pos[pv[0].tag.POS]\n",
    "                    else:\n",
    "                        word = pv[0].normal_form\n",
    "                    words.append(word)\n",
    "                    tmp_dict2[t] = word\n",
    "#                 else:\n",
    "#                     nones[t] = \"\"\n",
    "        output.append(words)\n",
    "        output = [s for s in output if len(s) > 1]\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for text in tqdm_notebook(df_positive['Context'].dropna()):\n",
    "    if text == 0:\n",
    "        continue\n",
    "    all_sentences.extend(normalizePymorphy_sentences(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(sentences=all_sentences, min_count=5, threshold=10)\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "for index, sentence in enumerate(all_sentences):\n",
    "    all_sentences[index] = bigram[sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300\n",
    "min_word_count = 5  \n",
    "num_workers = 4 \n",
    "context = 5          \n",
    "downsampling = 1e-3  \n",
    "\n",
    "w2v_model = Word2Vec(all_sentences, workers=8, size=num_features,\n",
    "                 min_count=min_word_count, window=context, sample=downsampling, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_corpus(sentences):\n",
    "    shuffled = list(sentences)\n",
    "    random.shuffle(shuffled)\n",
    "    return shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.build_vocab(sentences=shuffle_corpus(all_sentences), update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_model.train(sentences=shuffle_corpus(all_sentences),\n",
    "                epochs=5, total_examples=w2v_model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.corpus_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.most_similar('кодак')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "w2v_dict = dict(zip(w2v_model.wv.index2word, w2v_model.wv.syn0))\n",
    "tfidf = TfidfVectorizer(norm=None).fit(df_train['NContext'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 300\n",
    "window_size = 5\n",
    "min_word = 5\n",
    "down_sampling = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_model = FastText(all_sentences,\n",
    "                          size=embedding_size,\n",
    "                          window=window_size,\n",
    "                          min_count=min_word,\n",
    "                          sample=down_sampling,\n",
    "                          sg=1,\n",
    "                          workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastText_model.wv.most_similar(['кодак'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_recall(y, y_test, k=1):\n",
    "    num_examples = float(len(y))\n",
    "    num_correct = 0\n",
    "    for predictions, label in zip(y, y_test):\n",
    "        if label in predictions[:k]:\n",
    "            num_correct += 1\n",
    "    return num_correct/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class TFIDFPredictor:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.vectorizer = TfidfVectorizer().fit(data.values)\n",
    "\n",
    "    def predict(self, context, utterances):\n",
    "        vector_context = self.vectorizer.transform([context])\n",
    "        vector_uttr = self.vectorizer.transform(utterances)\n",
    "        # Длина векторов tfidf равна единице (vectore magnitude), \n",
    "        # поэтому косинусное расстояние = скалярное произведение, которое можно посчитать как ниже \n",
    "        #(linear_kernel - попарное скалярное произведение),\n",
    "        # такой подход ускорит работу, вместо cosine_similarity, не нужно подсчитывать зря длину векторов:\n",
    "        # result = linear_kernel(vector_context, vector_uttr).flatten()\n",
    "        result = np.dot(vector_uttr, vector_context.T).todense()\n",
    "        result = np.asarray(result).flatten()\n",
    "        return np.argsort(result, axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_model = TFIDFPredictor()\n",
    "tfidf_model.fit(df_train['NContext'])\n",
    "y = [tfidf_model.predict(df_test['NContext'][x], df_test.iloc[x, 3]) for x in tqdm_notebook(range(len(df_test)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.zeros(df_test.shape[0])\n",
    "for n in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y, y_test, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.wv.vocab:\n",
    "            mean.append(wv.wv.syn0norm[wv.wv.vocab[word].index])\n",
    "            all_words.add(wv.wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class W2VPredictor: \n",
    "    def __init__(self):\n",
    "        ''\n",
    "\n",
    "    def fit(self, data):\n",
    "        ''\n",
    "\n",
    "    def predict(self, context, utterances):\n",
    "        vector_context = word_averaging_list(w2v_model, [context.split()])\n",
    "        vector_uttr = word_averaging_list(w2v_model, [x.split() for x in utterances])\n",
    "        result = cosine_similarity(vector_context, vector_uttr)\n",
    "        result = np.asarray(result).flatten()\n",
    "        return np.flip(np.argsort(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_model = W2VPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = [w2vec_model.predict(df_test['NContext'][x], df_test.iloc[x, 3]) for x in tqdm_notebook(range(len(df_test)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y, y_test, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with TF-IDF & Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "max_idf = max(tfidf.idf_)\n",
    "weights = defaultdict(lambda: max_idf, [(\n",
    "            w, tfidf.idf_[i]) for w, i in tqdm_notebook(tfidf.vocabulary_.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedW2VPredictor:\n",
    "    def __init__(self, tfidf, num_features, weights):\n",
    "        self.max_idf = max(tfidf.idf_)\n",
    "        self.weights = weights\n",
    "        self.dim = num_features\n",
    "\n",
    "    def fit(self, tfidf, data):\n",
    "        ''\n",
    "\n",
    "    def predict(self, w2v_model, context, utterances):\n",
    "        vector_context = np.array([np.mean([w2v_model[w] * self.weights[w] for w in context.split() if w in w2v_model]\n",
    "                                or [np.zeros(self.dim)], axis=0)])\n",
    "    \n",
    "        vector_uttr = np.array([np.mean([w2v_model[w] * self.weights[w] for w in words.split() if w in w2v_model] or\n",
    "                                        [np.zeros(self.dim)], axis=0) for words in utterances])\n",
    "        result = cosine_similarity(vector_context[0].reshape(1, -1), vector_uttr)\n",
    "        result = np.asarray(result).flatten()\n",
    "\n",
    "        return np.flip(np.argsort(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_w2v_model = WeightedW2VPredictor(tfidf, num_features, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [w_w2v_model.predict(w2v_model, df_test['NContext'][x], df_test.iloc[x, 3])\n",
    "     for x in tqdm_notebook(range(len(df_test)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y, y_test, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FTPredictor: \n",
    "    def __init__(self):\n",
    "        ''\n",
    "\n",
    "    def fit(self, data):\n",
    "        ''\n",
    "\n",
    "    def predict(self, context, utterances):\n",
    "        vector_context = word_averaging_list(fastText_model, [context.split()])\n",
    "        vector_uttr = word_averaging_list(fastText_model, [x.split() for x in utterances])\n",
    "        result = cosine_similarity(vector_context, vector_uttr)\n",
    "        result = np.asarray(result).flatten()\n",
    "        return np.flip(np.argsort(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastT_model = FTPredictor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [fastT_model.predict(df_test['NContext'][x], df_test.iloc[x, 3]) for x in tqdm_notebook(range(len(df_test)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y, y_test, n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with LASER embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class LASERPredictor:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = Laser()\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.vectorizer.embed_sentences(data, lang='ru')\n",
    "\n",
    "    def predict(self, context, utterances):\n",
    "        vector_context = self.vectorizer.embed_sentences([context], lang='ru')\n",
    "        vector_uttr = self.vectorizer.embed_sentences(utterances, lang='ru')\n",
    "        result = cosine_similarity(vector_context, vector_uttr)\n",
    "        result = np.asarray(result).flatten()\n",
    "        return np.flip(np.argsort(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser_model = LASERPredictor()\n",
    "laser_model.fit(df_train['Context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = [laser_model.predict(df_test['NContext'][x], df_test.iloc[x, 3]) for x in tqdm_notebook(range(len(df_test)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [1, 2, 5, 10]:\n",
    "    print(\"Recall @ ({}, 10): {:g}\".format(n, evaluate_recall(y, y_test, n)))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "720px",
    "left": "28px",
    "top": "110px",
    "width": "230px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
